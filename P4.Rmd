---
title: "Project 4"
author: "Brad Harbans"
date: "5/2/2021"
output: html_document
bibliography: ref.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(SnowballC)
library(rvest)
library(wordcloud)
library(reactable)
```

## Introduction

For this assignment we have given two sets of email messages. One set is known to be spam and another set is known to be "ham", a legitimate message. I have downloaded two files from the example corpus https://spamassassin.apache.org/old/publiccorpus/ ^[@old_publiccorpus_2004]. I have downloaded the files named `20021010_easy_ham.tar.bz2` and `20021010_spam.tar.bz2` containg sample ham and spam messages respectively. I have also made my chosen files available on [github](https://github.com/bharbans/DATA607_P4/tree/main/corpus). 

## Import Data  {.tabset}

### Text Mining Package

I will be using the `tm`, Text Mining Package, in R to import the spam and ham data sets and to do some analysis.

```{r message=FALSE}
library(tm)
```

### Download and Unzip Files

```{r, eval=F}
hamURL <- 'https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2'
spamURL <- 'https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2'

download.file(hamURL,"corpus/20021010_easy_ham.tar.bz2")
download.file(spamURL,"corpus/20021010_spam.tar.bz2")

untar("corpus/20021010_easy_ham.tar.bz2", exdir = "corpus/")
untar("corpus/20021010_spam.tar.bz2", exdir = "corpus/")
```


### Import Data from Files and Export to Data Frame
The data sets have been imported as as `Corpus` objects. I have converted these `Corpus` objects to data frames in order to manipulate the data.  
```{r}
spamCorpus <- Corpus(DirSource(directory = "corpus/spam", encoding = "ASCII"))
hamCorpus <- Corpus(DirSource(directory = "corpus/easy_ham/",encoding = "ASCII"))

spam <- data.frame(text = sapply(spamCorpus, as.character), stringsAsFactors = FALSE)
ham <- data.frame(text = sapply(hamCorpus, as.character), stringsAsFactors = FALSE)
```

### Combine Data Sets
```{r}
spam <- spam %>% 
  rownames_to_column("message-id") %>% 
  rename( message=text ) %>% 
  mutate ( isSpam = 1)

ham <- ham %>% 
  rownames_to_column("message-id") %>% 
  rename( message=text ) %>% 
  mutate ( isSpam = 0)
```

The body of a mail message comes after the header and consists of everything that follows the first blank line. ^[@costales_2002]. As a result I will split the message column by a field containing two consecutive new line characters.
```{r, warning=F}
combinedDataSet <- rbind(spam,ham) %>% 
  separate(message,sep = "(\r\n|\r|\n)(\r\n|\r|\n)", into = c("headers","body"), extra = "merge")
```

### Separate Useful Information into Columns
I will now strip any HTML tags from the message, using a regular expression. 
```{r}
combinedDataSet <- combinedDataSet %>% 
  mutate( body_plaintext = str_replace_all(body,"</?[^>]+>","") )

```

I would also like to look at the originating IP address from the header. Please note that since this data is from 2004, I am only matching IPv4 addresses, if this were to be used today IPv6 address should also be matched. The regex below was adapted from the following [website](https://www.bigdatamark.com/regexp-for-extracting-public-ip-address/)^[@mark_2016]. It will exclude any private and loopback IP addresses. The `str_extract` function on the headers with this regex should give us the first public IP address that the message passed through.

```{r}
regex <-"\\b(?!(10)|(127)|192\\.168|172\\.(2[0-9]|1[6-9]|3[0-2]))[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}" 

combinedDataSet <- combinedDataSet %>% 
  mutate( originatingIP = str_extract(headers,regex)  )
```


<!--There is some interesting information that can be obtained from the originating IP address of the message. I will extract that information from the ipgeolocation.io API.  -->

### Resultant DataFrame
The data frame now looks like this:
```{r}
combinedDataSet %>% 
  head(30) %>% 
  reactable(wrap = F)
```


## Creating Corpus{.tabset}

### Create Corpus

We will now create the corpus based on the data frame created in th previous steps.
```{r}
combinedDataFrameSource <- combinedDataSet %>% 
  select( `message-id`,body_plaintext) %>% 
  rename(`doc_id`= `message-id`, text = body_plaintext ) %>% 
  DataframeSource()

spamCorpus <-Corpus(combinedDataFrameSource)
```

### Data Pre-Processing
I will perform some pre-processing on the raw data.

```{r warning=FALSE}
spamCorpus <- spamCorpus %>% 
  tm_map(content_transformer(tolower))%>% 
  tm_map(removeNumbers) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(stripWhitespace) %>% 
  tm_map(removeWords, stopwords("SMART")) %>% 
  tm_map(removeWords, c("nbsp","email")) %>% 
  tm_map(stemDocument) 

```

### Example of One Document
An example of one document that has been stemmed, this requires the `SnowballC` package.
```{r}
writeLines(as.character(spamCorpus[[20]])) 
```

### Create Document Matrix
I will now create a `DocumentTermMatrix`. 
```{r}
spamDTM <- DocumentTermMatrix(spamCorpus) %>% removeSparseTerms(sparse = .2)

```

### Create Word Cloud - Spam
I will now create a word cloud containing the frequency of common words in the spam. 
```{r warning=FALSE}
spamIndex <- which( combinedDataSet$isSpam == 1 )
wordcloud( spamCorpus[spamIndex], min.freq = 200 )
```

### Create Word Cloud - Ham
I will now create a word cloud containing the frequency of common words in the spam. 
```{r warning=FALSE}
spamIndex <- which( combinedDataSet$isSpam == 0 )
wordcloud( spamCorpus[spamIndex], min.freq = 200)

```

## Model Using TidyModels Package{.tabset}


### Load Tidymodel Package
```{r message=FALSE}
library(tidymodels)
library(textrecipes)
library(stopwords)
library(naivebayes)
library(discrim)
```


### Split Data

I will begin by using the `rsample` package to split the data into a testing and training set.

```{r}
set.seed(123124) 

combinedDataSetForModel <- combinedDataSet %>% 
  select(`message-id`, body_plaintext, isSpam , originatingIP) %>% 
  mutate(isSpam = factor(isSpam) )


combinedDataSetSplit <- initial_split(combinedDataSetForModel, strata = "isSpam", p = 0.75)
train_data <- training(combinedDataSetSplit)
test_data <- testing(combinedDataSetSplit)
```

### Update Recipe
This recipe will tokenize the body field, and will keep words that appear more than 100 times. This recipe has been adapted from the following [post](https://juliasilge.com/blog/animal-crossing/)^[@silge_2020].
```{r}
spam_rec <-
  recipe(isSpam ~ . , data=train_data) %>% 
  update_role("message-id",new_role = "ID") %>% 
  step_naomit(all_predictors()) %>% 
  step_tokenize(body_plaintext) %>%
  step_stopwords(body_plaintext, keep = F) %>%
  step_tokenfilter(body_plaintext, max_tokens = 500) %>% 
  step_tfidf(body_plaintext) %>% 
  prep(training = train_data)
  
```

### Juice the Train Data and Bake the Test Data
```{r}
rec_train_data <- juice(spam_rec)
rec_test_data <- bake(spam_rec, test_data)
```

### Create and Fit Model
```{r}

model <- naive_Bayes(Laplace = 1) %>% 
  set_mode("classification") %>%
  set_engine("naivebayes") %>% 
  translate() %>% 
  fit(isSpam ~ .,data = rec_train_data  )
```

### Verify Model Accuracy
Please see the below confusion matrix for the model.
```{r}
testPred <- model %>% 
  predict( rec_test_data ) %>% 
  bind_cols(rec_test_data %>% select(isSpam))

testPred %>%
  conf_mat(isSpam, .pred_class) %>% 
  autoplot()

```


Please find the accuracy for the test data below.
```{r}
testPred %>%
  metrics(isSpam, .pred_class) %>%
  filter(.metric == "accuracy") %>% 
  select(-.estimator) %>% 
  reactable()
  
```

## Looking at Originating IP Address{.tabset}

### Obtaining IP information

I obtained the following function from the following [blog](https://heuristically.wordpress.com/2013/05/20/geolocate-ip-addresses-in-r/) post^[@ziem_2013]. It is a simple recursive function that takes an IP address or a vector of IP addresses and returns data about them using a JSON API for [freegeoip.app](https://freegeoip.app/). 
```{r}

freegeoip <- function(ip, format = ifelse(length(ip)==1,'list','dataframe'))
{
    if (1 == length(ip))
    {
        # a single IP address
        require(rjson)
        url <- paste(c("https://freegeoip.app/json/", ip), collapse='')
        ret <- fromJSON(readLines(url, warn=FALSE))
        if (format == 'dataframe')
            ret <- data.frame(t(unlist(ret)))
        return(ret)
    } else {
        ret <- data.frame()
        for (i in 1:length(ip))
        {
            r <- freegeoip(ip[i], format="dataframe")
            ret <- rbind(ret, r)
        }
        return(ret)
    }
}   
```

There is a limitation as to the number of queries that can be run on the API, as such I have obtained the unique IPs from the data sets, and will pass this to the `freegeoip` function above. *N.B.* The sample data consists of data from the year 2002, the API is providing current information, since then the records may have changed.
```{r} 
uniqueIPInfo <- combinedDataSet$originatingIP %>% 
  unique() %>% 
  na.omit() %>%  
  freegeoip()
```

I will now join the data with original data set.
```{r}
IPDataSet <- combinedDataSet %>% 
  left_join(uniqueIPInfo, by=c("originatingIP" = "ip") ) %>% 
  select( -headers, -body, -body_plaintext)

```

Preview of Resultant data set.
```{r}
IPDataSet %>% 
  head(30) %>% 
  reactable(wrap = F)
```
### Looking at the messages based on Location

```{r}
countryDataSet <- IPDataSet %>% 
  select(-`message-id`,country_name, isSpam, originatingIP) %>% 
  na.omit()

countryDataSet %>%
  head(50) %>% 
  reactable(wrap=F)
```

### Summary Statistics for Region
```{r message=FALSE, warning=FALSE}
library(epiDisplay)
```

```{r}
spamByCountry <- countryDataSet %>% 
  filter(isSpam == 1)

hamByCountry <- countryDataSet %>% 
  filter(isSpam == 0)

tab1(spamByCountry$region_name, sort.group = "decreasing", cum.percent = TRUE ,main = "Spam By Region Name" )
tab1(hamByCountry$region_name, sort.group = "decreasing", cum.percent = TRUE ,main = "Ham By Region Name" )
```

### Summary Statistics for Country
```{r}
tab1(spamByCountry$country_name, sort.group = "decreasing", cum.percent = TRUE ,main = "Spam By Country" )
tab1(hamByCountry$country_name, sort.group = "decreasing", cum.percent = TRUE ,main = "Ham By Country" )
```

<!-- <!-- -->
<!-- ### Split Dataset -->
<!-- ```{r} -->

<!-- set.seed(12312) -->

<!-- countryDataSetSplit <- initial_split(countryDataSet, strata = "isSpam", p = 0.75) -->
<!-- train_data_country <- training(countryDataSetSplit) -->
<!-- test_data_country <- testing(countryDataSetSplit) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- lm_model <- linear_reg() %>%  -->
<!--             set_engine('lm') %>% # adds lm implementation of linear regression -->
<!--             set_mode('regression') -->
<!-- ``` -->

<!-- I had to re-level the factor for the test data as it contains countries that do not exist in the training data. These will be converted to `NA`. -->
<!-- ```{r} -->

<!-- #test_data_country$country_name <- factor(test_data_country$country_name,levels = levels(train_data_country$country_name))  -->

<!-- lm_fit <- lm_model %>%  -->
<!--           fit(isSpam ~ . , data = train_data_country) -->

<!-- tidy(lm_fit) %>% reactable() -->
<!-- ``` -->

<!-- ```{r} -->
<!-- testPred <- lm_fit %>%  -->
<!--   predict( test_data_country) %>%  -->
<!--   bind_cols(test_data_country %>% select(isSpam)) -->

<!-- #testPred %>% -->
<!-- #  conf_mat(truth = isSpam, estimate =.pred ) %>%  -->

<!-- ``` -->
<!-- --> -->
 


## References


